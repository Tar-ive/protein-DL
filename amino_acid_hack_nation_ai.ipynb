{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNsEPB4UFBnzvSYiu8ZaJim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/protein-DL/blob/main/amino_acid_hack_nation_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVFeyv7HD1WQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"googleai/pfam-seed-random-split\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "TAdEf-vgEPeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "rcEgOdylEcg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check what's actually in the dataset directory\n",
        "print(\"Contents of dataset directory:\")\n",
        "for item in os.listdir(path):\n",
        "    print(f\"  {item}\")\n",
        "    if os.path.isdir(os.path.join(path, item)):\n",
        "        print(f\"    Contents of {item}:\")\n",
        "        for subitem in os.listdir(os.path.join(path, item)):\n",
        "            print(f\"      {subitem}\")"
      ],
      "metadata": {
        "id": "uxg-H_S3EZUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_path = os.path.join(path, 'random_split', 'random_split')\n"
      ],
      "metadata": {
        "id": "dVdxYDulOffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_sharded_files(subdir_name, base_path):\n",
        "    \"\"\"Read all sharded data files from a subdirectory and concatenate them\"\"\"\n",
        "    dir_path = os.path.join(base_path, subdir_name)\n",
        "    data_frames = []\n",
        "\n",
        "    # Get all files and sort them to maintain order\n",
        "    files = sorted([f for f in os.listdir(dir_path) if f.startswith('data-')])\n",
        "\n",
        "    for file in files:\n",
        "        file_path = os.path.join(dir_path, file)\n",
        "        try:\n",
        "            # Try reading as parquet first (most likely format)\n",
        "            df = pd.read_parquet(file_path)\n",
        "            data_frames.append(df)\n",
        "        except:\n",
        "            try:\n",
        "                # If parquet fails, try as CSV\n",
        "                df = pd.read_csv(file_path)\n",
        "                data_frames.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "    if data_frames:\n",
        "        return pd.concat(data_frames, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"No readable files found in {dir_path}\")\n",
        "        return None\n",
        "\n",
        "# Use the new function to load your data\n",
        "train = read_data_from_sharded_files('train', inner_path)\n",
        "dev = read_data_from_sharded_files('dev', inner_path)\n",
        "test = read_data_from_sharded_files('test', inner_path)\n",
        "\n",
        "print(f\"Train shape: {train.shape if train is not None else 'Failed to load'}\")\n",
        "print(f\"Dev shape: {dev.shape if dev is not None else 'Failed to load'}\")\n",
        "print(f\"Test shape: {test.shape if test is not None else 'Failed to load'}\")"
      ],
      "metadata": {
        "id": "zIvn7BR1Ed4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()\n"
      ],
      "metadata": {
        "id": "XPTA_z-CE6Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "1tczOIu1E-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev.shape"
      ],
      "metadata": {
        "id": "6i4w7XtwE-uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "CELUOkwXE_-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at families in the training data"
      ],
      "metadata": {
        "id": "YOeUCYsZFR-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partitions = {'train': train, 'dev': dev, 'test': test}\n"
      ],
      "metadata": {
        "id": "Iz8ugCYnF0e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_information(partitions):\n",
        "    columns = ['partition', 'nb_samples', 'nb_families', 'min_samples_per_fam', 'max_samples_per_fam', 'mean_samples_per_fam']\n",
        "    df_info = pd.DataFrame(columns=columns)\n",
        "    for name, df in partitions.items():\n",
        "        # Use pd.concat instead of df.append\n",
        "        df_info = pd.concat([df_info, pd.DataFrame([{\n",
        "            'partition': name,\n",
        "            'nb_samples': len(df),\n",
        "            'nb_families': df['family_accession'].unique().size,\n",
        "            'max_samples_per_fam': df.groupby('family_accession').size().max(),\n",
        "            'min_samples_per_fam': df.groupby('family_accession').size().min(),\n",
        "            'mean_samples_per_fam': df.groupby('family_accession').size().mean(),\n",
        "        }])], ignore_index=True)\n",
        "    return df_info\n",
        "\n",
        "get_information(partitions)"
      ],
      "metadata": {
        "id": "mS99cQbnFx0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_families = set(train['family_accession'].unique())\n",
        "dev_families = set(dev['family_accession'].unique())\n",
        "test_families = set(test['family_accession'].unique())\n",
        "print('Are the families of the dev set and the test set the same ?', dev_families == test_families)\n",
        "\n",
        "common_families = train_families & dev_families & test_families # Take the intersection with the '&' operator\n",
        "print('Number of common families in all sets : ', len(common_families))"
      ],
      "metadata": {
        "id": "Qo8jPPBaFAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excluding the families that are only in train but not in dev and test\n"
      ],
      "metadata": {
        "id": "UfQpvPOTFizj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train['family_accession'].isin(common_families)]\n",
        "partitions['train'] = train\n",
        "\n",
        "print('Updated info on the datasets')\n",
        "get_information(partitions)"
      ],
      "metadata": {
        "id": "AbJGnMn6FiJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (30, 10))\n",
        "plt.suptitle('Distribution of family sizes', fontsize=18, y=0.95)\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "for n, (name, df) in enumerate(partitions.items()):\n",
        "    # Create the subpot\n",
        "    ax = plt.subplot(1, 3, n + 1)\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(\"Family size\")\n",
        "    ax.set_ylabel(\"Number of families\")\n",
        "\n",
        "    # Plot data\n",
        "    df.groupby('family_id').size().hist(bins=100, ax=ax, color=colors[n])\n"
      ],
      "metadata": {
        "id": "4cKgiXA9F9zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Environemnt Setup"
      ],
      "metadata": {
        "id": "1iHvDxb5Ko03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets evaluate scikit-learn\n"
      ],
      "metadata": {
        "id": "KT92TweGKsnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zJqHilZpKwem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "NzDzvYAsLI45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You already have this data loaded, so let's just verify it\n",
        "print(\"Data shapes:\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Dev: {dev.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "\n",
        "# Check the column names\n",
        "print(f\"\\nTrain columns: {train.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "8VeAFe4yLKos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all datasets for sampling strategy\n",
        "all_data = pd.concat([train, dev, test], ignore_index=True)\n",
        "print(f\"Total dataset size: {all_data.shape}\")\n",
        "\n",
        "# Explore the family distribution\n",
        "family_counts = all_data['family_accession'].value_counts()\n",
        "print(f\"Number of unique families: {len(family_counts)}\")\n",
        "print(f\"Most common families:\")\n",
        "print(family_counts.head(10))"
      ],
      "metadata": {
        "id": "UJC8gZYiLNUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed things up, I selected a smaller, representative sample. A good starting point was to take the top 1,000 most frequent families and then take up to 100 examples from each of those families. This gives me a balanced and manageable dataset of around 100,000 sequences"
      ],
      "metadata": {
        "id": "0Pkoobu2LRT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 1000 most frequent families\n",
        "top_1000_families = family_counts.head(1000).index.tolist()\n",
        "print(f\"Selected top {len(top_1000_families)} families\")\n",
        "\n",
        "# Filter data to only include these families\n",
        "filtered_data = all_data[all_data['family_accession'].isin(top_1000_families)]\n",
        "print(f\"Filtered dataset size: {filtered_data.shape}\")"
      ],
      "metadata": {
        "id": "X8NGva2hLfGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample up to 100 sequences per family for balanced training\n",
        "sampled_data = []\n",
        "\n",
        "for family in top_1000_families:\n",
        "    family_data = filtered_data[filtered_data['family_accession'] == family]\n",
        "    # Sample up to 100, or all if less than 100\n",
        "    sample_size = min(100, len(family_data))\n",
        "    sampled_family = family_data.sample(n=sample_size, random_state=42)\n",
        "    sampled_data.append(sampled_family)\n",
        "\n",
        "# Combine all sampled data\n",
        "balanced_dataset = pd.concat(sampled_data, ignore_index=True)\n",
        "print(f\"Balanced dataset size: {balanced_dataset.shape}\")\n",
        "print(f\"Average samples per family: {len(balanced_dataset) / len(top_1000_families):.1f}\")"
      ],
      "metadata": {
        "id": "XpFj8xZlLkFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to the pickle file\n",
        "pickle_file_path = '/content/original_1k_training_data.pkl'\n",
        "csv_file_path = '/content/original_1k_training_data.csv'\n",
        "\n",
        "try:\n",
        "    # Load the data from the pickle file\n",
        "    with open(pickle_file_path, 'rb') as f:\n",
        "        loaded_data = pickle.load(f)\n",
        "\n",
        "    # Assuming the dataframe is stored under the key 'original_training_df'\n",
        "    if 'original_training_df' in loaded_data:\n",
        "        original_training_df = loaded_data['original_training_df']\n",
        "\n",
        "        # Save the dataframe to a CSV file\n",
        "        original_training_df.to_csv(csv_file_path, index=False)\n",
        "        print(f\"‚úÖ Dataframe saved to {csv_file_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Could not find 'original_training_df' key in the pickle file.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Pickle file not found at {pickle_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "pNcd8msMrWFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sequences and labels\n",
        "sequences = balanced_dataset['sequence'].tolist()\n",
        "family_labels = balanced_dataset['family_accession'].tolist()\n",
        "\n",
        "print(f\"Number of sequences: {len(sequences)}\")\n",
        "print(f\"Number of labels: {len(family_labels)}\")\n",
        "print(f\"Example sequence length: {len(sequences[0])}\")\n",
        "print(f\"Example sequence: {sequences[0][:50]}...\")"
      ],
      "metadata": {
        "id": "l6QJZo8qLxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert family accession strings to numbers\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(family_labels)\n",
        "\n",
        "print(f\"Label encoding complete!\")\n",
        "print(f\"Number of unique labels: {len(label_encoder.classes_)}\")\n",
        "print(f\"Example mappings:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {family_labels[i]} -> {encoded_labels[i]}\")"
      ],
      "metadata": {
        "id": "Huca6TfEL3rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ESM-2 tokenizer\n",
        "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_checkpoint}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
      ],
      "metadata": {
        "id": "F3asofyDL7Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all sequences (this might take a few minutes)\n",
        "print(\"Tokenizing sequences...\")\n",
        "tokenized_sequences = tokenizer(\n",
        "    sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,  # Adjust if needed based on your sequence lengths\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"Input shape: {tokenized_sequences['input_ids'].shape}\")"
      ],
      "metadata": {
        "id": "gpcY7hVAMBvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets (80/20 split)\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    sequences,\n",
        "    encoded_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=encoded_labels  # Ensure balanced split across families\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_sequences)}\")\n",
        "print(f\"Test set size: {len(test_sequences)}\")"
      ],
      "metadata": {
        "id": "k46X5-MXMW5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the split data\n",
        "train_tokenized = tokenizer(\n",
        "    train_sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "test_tokenized = tokenizer(\n",
        "    test_sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "print(\"Split data tokenized!\")"
      ],
      "metadata": {
        "id": "ScmF8TDpMenV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_tokenized)\n",
        "test_dataset = Dataset.from_dict(test_tokenized)\n",
        "\n",
        "# Add labels\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels.tolist())\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels.tolist())\n",
        "\n",
        "print(\"Final datasets created!\")\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Test dataset: {test_dataset}\")\n",
        "print(f\"Number of labels: {len(label_encoder.classes_)}\")"
      ],
      "metadata": {
        "id": "yDSx_pQLMuk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ESM-2 model for sequence classification\n",
        "num_labels = 1000  # Your number of protein families\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "print(f\"Model loaded with {num_labels} output classes\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "b6UybGngMzc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face to enable automatic upload\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "esGBhs78NrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Create a descriptive model name\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "output_dir = f\"{model_name}-finetuned-pfam-1k\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,  # Adjust if you get memory errors\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,  # This will auto-upload to HF!\n",
        "    hub_model_id=f\"Tarive/{output_dir}\",  # Replace with your HF username\n",
        "    hub_strategy=\"every_save\",\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "print(f\"Training will save to: {output_dir}\")\n",
        "print(f\"Model will be uploaded to: Tarive/{output_dir}\")"
      ],
      "metadata": {
        "id": "J7i3bd_0Nu1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# Load accuracy metric\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined!\")"
      ],
      "metadata": {
        "id": "eNafEGo5N9My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer created! Ready to start training...\")"
      ],
      "metadata": {
        "id": "-R7GMqUHN_rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the big moment - start training!\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"This will take approximately 15-30 minutes on T4 GPU\")\n",
        "print(\"You'll see progress bars and accuracy metrics\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training complete!\")"
      ],
      "metadata": {
        "id": "joo-8pfoOCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"googleai/pfam-seed-random-split\")\n",
        "\n",
        "# Define the path to the data\n",
        "inner_path = os.path.join(path, 'random_split', 'random_split')\n",
        "\n",
        "# Load the data using the defined function\n",
        "def read_data_from_sharded_files(subdir_name, base_path):\n",
        "    \"\"\"Read all sharded data files from a subdirectory and concatenate them\"\"\"\n",
        "    dir_path = os.path.join(base_path, subdir_name)\n",
        "    data_frames = []\n",
        "\n",
        "    # Get all files and sort them to maintain order\n",
        "    files = sorted([f for f in os.listdir(dir_path) if f.startswith('data-')])\n",
        "\n",
        "    for file in files:\n",
        "        file_path = os.path.join(dir_path, file)\n",
        "        try:\n",
        "            # Try reading as parquet first (most likely format)\n",
        "            df = pd.read_parquet(file_path)\n",
        "            data_frames.append(df)\n",
        "        except:\n",
        "            try:\n",
        "                # If parquet fails, try as CSV\n",
        "                df = pd.read_csv(file_path)\n",
        "                data_frames.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "    if data_frames:\n",
        "        return pd.concat(data_frames, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"No readable files found in {dir_path}\")\n",
        "        return None\n",
        "\n",
        "train = read_data_from_sharded_files('train', inner_path)\n",
        "dev = read_data_from_sharded_files('dev', inner_path)\n",
        "test = read_data_from_sharded_files('test', inner_path)\n",
        "\n",
        "\n",
        "# Combine all datasets for sampling strategy\n",
        "all_data = pd.concat([train, dev, test], ignore_index=True)\n",
        "\n",
        "# Recreate balanced_dataset\n",
        "# Get top 1000 most frequent families\n",
        "family_counts = all_data['family_accession'].value_counts()\n",
        "top_1000_families = family_counts.head(1000).index.tolist()\n",
        "\n",
        "# Filter data to only include these families\n",
        "filtered_data = all_data[all_data['family_accession'].isin(top_1000_families)]\n",
        "\n",
        "# Sample up to 100 sequences per family for balanced training\n",
        "sampled_data = []\n",
        "\n",
        "for family in top_1000_families:\n",
        "    family_data = filtered_data[filtered_data['family_accession'] == family]\n",
        "    # Sample up to 100, or all if less than 100\n",
        "    sample_size = min(100, len(family_data))\n",
        "    sampled_family = family_data.sample(n=sample_size, random_state=42)\n",
        "    sampled_data.append(sampled_family)\n",
        "\n",
        "# Combine all sampled data\n",
        "balanced_dataset = pd.concat(sampled_data, ignore_index=True)\n",
        "\n",
        "# Extract sequences and labels (make sure this is defined here or accessible)\n",
        "sequences = balanced_dataset['sequence'].tolist()\n",
        "family_labels = balanced_dataset['family_accession'].tolist()\n",
        "\n",
        "# Fit the label encoder before saving it\n",
        "label_encoder.fit(family_labels) # Fit the encoder here\n",
        "\n",
        "# Save the label encoder to a file named 'label_encoder.pkl'\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"‚úÖ LabelEncoder saved to label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "riMVCyEQlCBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zDvvZEaicLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the LabelEncoder from the file\n",
        "with open('label_encoder.pkl', 'rb') as f:\n",
        "    loaded_label_encoder = pickle.load(f)\n",
        "\n",
        "# Display the mapping\n",
        "print(\"Mapping of original labels to numerical labels:\")\n",
        "for i, label in enumerate(loaded_label_encoder.classes_):\n",
        "    print(f\"  {label} -> {i}\")"
      ],
      "metadata": {
        "id": "sDlX-Q0djN-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evals, evals, evals"
      ],
      "metadata": {
        "id": "svCWPkswWjXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "import pickle\n",
        "import requests\n",
        "from huggingface_hub import hf_hub_download\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# Your model repository\n",
        "model_repo = \"Tarive/esm2_t12_35M_UR50D-finetuned-pfam-1k\"\n",
        "\n",
        "print(\"üì• Loading your trained model from Hugging Face...\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_repo)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "# Download and load the label encoder\n",
        "label_encoder_path = hf_hub_download(repo_id=model_repo, filename=\"label_encoder.pkl\")\n",
        "with open(label_encoder_path, 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"Model: {model_repo}\")\n",
        "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
        "print(f\"Label encoder classes preview: {label_encoder.classes_[:10]}\")\n",
        "\n",
        "# Create a classification pipeline for easy inference\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "print(\"üî¨ Classification pipeline ready!\")"
      ],
      "metadata": {
        "id": "oxZsofNGL03G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Check if we already have the reconstructed data saved\n",
        "RECONSTRUCTED_DATA_FILE = 'original_1k_training_data.pkl'\n",
        "CLEAN_TEST_DATA_FILE = 'clean_test_data_50.pkl'\n",
        "\n",
        "if os.path.exists(RECONSTRUCTED_DATA_FILE) and os.path.exists(CLEAN_TEST_DATA_FILE):\n",
        "    print(\"üìÅ Loading previously reconstructed data from memory...\")\n",
        "\n",
        "    # Load saved data\n",
        "    with open(RECONSTRUCTED_DATA_FILE, 'rb') as f:\n",
        "        training_data_cache = pickle.load(f)\n",
        "\n",
        "    with open(CLEAN_TEST_DATA_FILE, 'rb') as f:\n",
        "        test_data_cache = pickle.load(f)\n",
        "\n",
        "    # Extract variables from cache\n",
        "    original_training_sequences = training_data_cache['original_training_sequences']\n",
        "    original_top_1000_families = training_data_cache['original_top_1000_families']\n",
        "    original_training_df = training_data_cache['original_training_df']\n",
        "\n",
        "    clean_test_df = test_data_cache['clean_test_df']\n",
        "    valid_test_data = test_data_cache['valid_test_data']\n",
        "\n",
        "    print(f\"‚úÖ Loaded cached data:\")\n",
        "    print(f\"  Training sequences: {len(original_training_sequences):,}\")\n",
        "    print(f\"  Training families: {len(original_top_1000_families)}\")\n",
        "    print(f\"  Test sequences: {len(valid_test_data)}\")\n",
        "    print(f\"  Unique families in test: {clean_test_df['family_accession'].nunique()}\")\n",
        "\n",
        "else:\n",
        "    print(\"üîç Reconstructing original 1,000-class training dataset (first time)...\")\n",
        "\n",
        "    # The original model was trained on:\n",
        "    # - Top 1,000 families\n",
        "    # - ~100 samples per family (balanced sampling)\n",
        "    # - Total ~100K sequences\n",
        "\n",
        "    # Get the exact same top 1,000 families that were used originally\n",
        "    family_counts = all_data_for_search['family_accession'].value_counts()\n",
        "    original_top_1000_families = family_counts.head(1000).index.tolist()\n",
        "\n",
        "    print(f\"üìä Original training dataset specification:\")\n",
        "    print(f\"  Top families: 1,000\")\n",
        "    print(f\"  Samples per family: ~100\")\n",
        "    print(f\"  Expected total: ~100,000 sequences\")\n",
        "\n",
        "    # Reconstruct the exact training dataset using the same sampling strategy\n",
        "    original_training_data = []\n",
        "\n",
        "    for family in original_top_1000_families:\n",
        "        family_data = all_data_for_search[all_data_for_search['family_accession'] == family]\n",
        "        # Sample up to 100, or all if less than 100 (same strategy as original)\n",
        "        sample_size = min(100, len(family_data))\n",
        "        sampled_family = family_data.sample(n=sample_size, random_state=42)  # Same random seed\n",
        "        original_training_data.append(sampled_family)\n",
        "\n",
        "    original_training_df = pd.concat(original_training_data, ignore_index=True)\n",
        "    original_training_sequences = set(original_training_df['sequence'].tolist())\n",
        "\n",
        "    print(f\"‚úÖ Reconstructed original training dataset:\")\n",
        "    print(f\"  Total sequences: {len(original_training_df):,}\")\n",
        "    print(f\"  Unique families: {original_training_df['family_accession'].nunique()}\")\n",
        "    print(f\"  Families covered: {original_training_df['family_accession'].nunique()}/1000\")\n",
        "\n",
        "    # Verify this matches the expected size\n",
        "    expected_size_range = (80000, 120000)  # 80K-120K range\n",
        "    if expected_size_range[0] <= len(original_training_df) <= expected_size_range[1]:\n",
        "        print(f\"‚úÖ Size verification passed: {len(original_training_df):,} sequences in expected range\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Size verification: {len(original_training_df):,} sequences (expected {expected_size_range[0]:,}-{expected_size_range[1]:,})\")\n",
        "\n",
        "    # Now create a clean test set excluding ALL original training sequences\n",
        "    print(f\"\\nüî¨ Creating clean test set excluding original training data...\")\n",
        "    clean_test_candidates = all_data_for_search[~all_data_for_search['sequence'].isin(original_training_sequences)]\n",
        "\n",
        "    print(f\"üìä Clean test candidate filtering:\")\n",
        "    print(f\"  Original dataset: {len(all_data_for_search):,} sequences\")\n",
        "    print(f\"  Training sequences to exclude: {len(original_training_sequences):,}\")\n",
        "    print(f\"  Clean candidates remaining: {len(clean_test_candidates):,}\")\n",
        "\n",
        "    # Create diverse test set from remaining data\n",
        "    target_test_size = 50\n",
        "\n",
        "    if len(clean_test_candidates) < target_test_size:\n",
        "        print(f\"‚ö†Ô∏è Only {len(clean_test_candidates)} clean candidates available\")\n",
        "        target_test_size = len(clean_test_candidates)\n",
        "\n",
        "    # Sample test sequences ensuring family diversity\n",
        "    test_family_counts = clean_test_candidates['family_accession'].value_counts()\n",
        "    test_sequences_list = []\n",
        "\n",
        "    # Strategy 1: Try to get sequences from families NOT in the training set\n",
        "    training_families = set(original_top_1000_families)\n",
        "    non_training_families = [f for f in test_family_counts.index if f not in training_families]\n",
        "\n",
        "    print(f\"üéØ Test set sampling strategy:\")\n",
        "    print(f\"  Families in training: {len(training_families)}\")\n",
        "    print(f\"  Families not in training: {len(non_training_families)}\")\n",
        "\n",
        "    # First priority: sequences from families not in training\n",
        "    if len(non_training_families) > 0:\n",
        "        print(f\"  Prioritizing families not seen during training...\")\n",
        "        for family in non_training_families[:target_test_size]:\n",
        "            if len(test_sequences_list) >= target_test_size:\n",
        "                break\n",
        "            family_data = clean_test_candidates[clean_test_candidates['family_accession'] == family]\n",
        "            if len(family_data) > 0:\n",
        "                sampled = family_data.sample(n=1, random_state=42)\n",
        "                test_sequences_list.append(sampled.iloc[0])\n",
        "\n",
        "    # Second priority: additional sequences from training families (but sequences not used in training)\n",
        "    remaining_needed = target_test_size - len(test_sequences_list)\n",
        "    if remaining_needed > 0:\n",
        "        print(f\"  Adding {remaining_needed} sequences from training families (unseen sequences)...\")\n",
        "        remaining_candidates = clean_test_candidates[\n",
        "            ~clean_test_candidates.index.isin([seq['name'] if isinstance(seq, dict) else i for i, seq in enumerate(test_sequences_list)])\n",
        "        ]\n",
        "        if len(remaining_candidates) > 0:\n",
        "            additional = remaining_candidates.sample(n=min(remaining_needed, len(remaining_candidates)), random_state=42)\n",
        "            for _, row in additional.iterrows():\n",
        "                test_sequences_list.append(row)\n",
        "\n",
        "    # Create final test DataFrame\n",
        "    clean_test_df = pd.DataFrame(test_sequences_list)\n",
        "\n",
        "    print(f\"\\n‚úÖ Final clean test set created:\")\n",
        "    print(f\"  Total test sequences: {len(clean_test_df)}\")\n",
        "    print(f\"  Unique families in test: {clean_test_df['family_accession'].nunique()}\")\n",
        "\n",
        "    # Analyze test set composition\n",
        "    test_families_in_training = clean_test_df['family_accession'].isin(training_families).sum()\n",
        "    test_families_not_in_training = len(clean_test_df) - test_families_in_training\n",
        "\n",
        "    print(f\"  Sequences from families in training: {test_families_in_training}\")\n",
        "    print(f\"  Sequences from families NOT in training: {test_families_not_in_training}\")\n",
        "\n",
        "    # Final verification: NO sequence overlap\n",
        "    overlap_check = clean_test_df['sequence'].isin(original_training_sequences).sum()\n",
        "    print(f\"üîç CRITICAL VERIFICATION: {overlap_check} test sequences overlap with training (MUST be 0)\")\n",
        "\n",
        "    if overlap_check == 0:\n",
        "        print(\"‚úÖ SUCCESS: Clean test set with no training data contamination!\")\n",
        "    else:\n",
        "        print(\"‚ùå ERROR: Test set contains training sequences!\")\n",
        "\n",
        "    # Prepare final test data\n",
        "    valid_test_data = [(row['sequence'], row['family_accession']) for _, row in clean_test_df.iterrows()]\n",
        "\n",
        "    print(f\"\\nüíæ Saving reconstructed data to memory for future use...\")\n",
        "\n",
        "    # Save training data\n",
        "    training_data_cache = {\n",
        "        'original_training_sequences': original_training_sequences,\n",
        "        'original_top_1000_families': original_top_1000_families,\n",
        "        'original_training_df': original_training_df\n",
        "    }\n",
        "\n",
        "    with open(RECONSTRUCTED_DATA_FILE, 'wb') as f:\n",
        "        pickle.dump(training_data_cache, f)\n",
        "\n",
        "    # Save test data\n",
        "    test_data_cache = {\n",
        "        'clean_test_df': clean_test_df,\n",
        "        'valid_test_data': valid_test_data\n",
        "    }\n",
        "\n",
        "    with open(CLEAN_TEST_DATA_FILE, 'wb') as f:\n",
        "        pickle.dump(test_data_cache, f)\n",
        "\n",
        "    print(f\"‚úÖ Data saved to:\")\n",
        "    print(f\"  üìÅ {RECONSTRUCTED_DATA_FILE}\")\n",
        "    print(f\"  üìÅ {CLEAN_TEST_DATA_FILE}\")\n",
        "\n",
        "print(f\"\\nüß™ Ready for evaluation:\")\n",
        "print(f\"  Test sequences: {len(valid_test_data)}\")\n",
        "print(f\"  Model was trained on: {len(original_training_sequences):,} sequences from {len(original_top_1000_families)} families\")\n",
        "print(f\"  Test set is completely clean of training data\")"
      ],
      "metadata": {
        "id": "VchibXqXXAF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sequences and true labels\n",
        "test_sequences_final = [item[0] for item in valid_test_data]\n",
        "true_families = [item[1] for item in valid_test_data]\n",
        "\n",
        "# CONFIGURABLE: Set how many sequences you want to test\n",
        "MAX_TEST_SEQUENCES = 50  # üéØ CHANGE THIS NUMBER TO TEST MORE/FEWER SEQUENCES\n",
        "\n",
        "# Limit to desired number of sequences\n",
        "if len(test_sequences_final) > MAX_TEST_SEQUENCES:\n",
        "    print(f\"üìä Limiting test set to {MAX_TEST_SEQUENCES} sequences (from {len(test_sequences_final)} available)\")\n",
        "    test_sequences_final = test_sequences_final[:MAX_TEST_SEQUENCES]\n",
        "    true_families = true_families[:MAX_TEST_SEQUENCES]\n",
        "else:\n",
        "    print(f\"üìä Using all {len(test_sequences_final)} available test sequences\")\n",
        "\n",
        "print(f\"üî¨ Running direct model predictions on {len(test_sequences_final)} sequences...\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "prediction_confidences = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, sequence in enumerate(test_sequences_final):\n",
        "        print(f\"Predicting sequence {i+1}/{len(test_sequences_final)}...\", end='\\r')\n",
        "\n",
        "        # Tokenize the sequence\n",
        "        inputs = tokenizer(\n",
        "            sequence,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Get probabilities and prediction\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_label_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "        confidence = probabilities[0, predicted_label_idx].item()\n",
        "\n",
        "        # Convert to family name\n",
        "        predicted_family = label_encoder.classes_[predicted_label_idx]\n",
        "\n",
        "        predictions.append(predicted_family)\n",
        "        prediction_confidences.append(confidence)\n",
        "\n",
        "print(f\"\\n‚úÖ Direct model predictions complete!\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'sequence': test_sequences_final,\n",
        "    'true_family': true_families,\n",
        "    'predicted_family': predictions,\n",
        "    'confidence': prediction_confidences,\n",
        "    'sequence_length': [len(seq) for seq in test_sequences_final]\n",
        "})\n",
        "\n",
        "# Add correctness column\n",
        "results_df['correct'] = results_df['true_family'] == results_df['predicted_family']\n",
        "\n",
        "print(f\"\\nüìä Prediction Results Summary:\")\n",
        "print(f\"Total predictions: {len(results_df)}\")\n",
        "print(f\"Correct predictions: {results_df['correct'].sum()}\")\n",
        "print(f\"Accuracy: {results_df['correct'].mean():.4f}\")\n",
        "print(f\"Average confidence: {results_df['confidence'].mean():.4f}\")\n",
        "\n",
        "# Show a few example predictions\n",
        "print(f\"\\nüîç Example Predictions:\")\n",
        "for i in range(min(5, len(results_df))):\n",
        "    row = results_df.iloc[i]\n",
        "    status = \"‚úÖ\" if row['correct'] else \"‚ùå\"\n",
        "    print(f\"{status} Sequence {i+1}: {row['predicted_family']} (confidence: {row['confidence']:.4f})\")\n",
        "    print(f\"   True: {row['true_family']}\")\n",
        "    print(f\"   Seq: {row['sequence'][:50]}...\")"
      ],
      "metadata": {
        "id": "S3lK2467XDeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate detailed metrics for all test sequences\n",
        "accuracy = accuracy_score(true_families, predictions)\n",
        "print(f\"üéØ DETAILED EVALUATION RESULTS - {len(results_df)} Test Sequences\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Number of test sequences: {len(results_df)}\")\n",
        "print(f\"Average prediction confidence: {results_df['confidence'].mean():.4f}\")\n",
        "print(f\"Confidence std deviation: {results_df['confidence'].std():.4f}\")\n",
        "\n",
        "# Confidence distribution analysis\n",
        "high_confidence = (results_df['confidence'] >= 0.8).sum()\n",
        "medium_confidence = ((results_df['confidence'] >= 0.6) & (results_df['confidence'] < 0.8)).sum()\n",
        "low_confidence = (results_df['confidence'] < 0.6).sum()\n",
        "\n",
        "print(f\"\\nüìä Confidence Distribution:\")\n",
        "print(f\"  High confidence (‚â•0.8): {high_confidence} ({high_confidence/len(results_df)*100:.1f}%)\")\n",
        "print(f\"  Medium confidence (0.6-0.8): {medium_confidence} ({medium_confidence/len(results_df)*100:.1f}%)\")\n",
        "print(f\"  Low confidence (<0.6): {low_confidence} ({low_confidence/len(results_df)*100:.1f}%)\")\n",
        "\n",
        "# Show sample of results (first 10 and last 5)\n",
        "print(f\"\\nüìã Sample Prediction Results:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# First 10 results\n",
        "for idx in range(min(10, len(results_df))):\n",
        "    row = results_df.iloc[idx]\n",
        "    status = \"‚úÖ CORRECT\" if row['correct'] else \"‚ùå INCORRECT\"\n",
        "    print(f\"Seq {idx+1:2d} ({row['sequence_length']:3d} aa): {status}\")\n",
        "    print(f\"      True: {row['true_family']}\")\n",
        "    print(f\"      Pred: {row['predicted_family']} (conf: {row['confidence']:.4f})\")\n",
        "\n",
        "if len(results_df) > 10:\n",
        "    print(f\"  ... [showing first 10 of {len(results_df)} total] ...\")\n",
        "\n",
        "    # Last 5 results\n",
        "    print(f\"\\nüìã Last 5 Results:\")\n",
        "    for idx in range(max(0, len(results_df)-5), len(results_df)):\n",
        "        row = results_df.iloc[idx]\n",
        "        status = \"‚úÖ CORRECT\" if row['correct'] else \"‚ùå INCORRECT\"\n",
        "        print(f\"Seq {idx+1:2d} ({row['sequence_length']:3d} aa): {status}\")\n",
        "        print(f\"      True: {row['true_family']}\")\n",
        "        print(f\"      Pred: {row['predicted_family']} (conf: {row['confidence']:.4f})\")\n",
        "\n",
        "# Analyze confidence vs accuracy correlation\n",
        "correct_confidences = results_df[results_df['correct']]['confidence']\n",
        "incorrect_confidences = results_df[~results_df['correct']]['confidence']\n",
        "\n",
        "print(f\"\\nüìä Confidence Analysis:\")\n",
        "if len(correct_confidences) > 0:\n",
        "    print(f\"Average confidence for correct predictions: {correct_confidences.mean():.4f}\")\n",
        "    print(f\"Min/Max confidence for correct: {correct_confidences.min():.4f}/{correct_confidences.max():.4f}\")\n",
        "if len(incorrect_confidences) > 0:\n",
        "    print(f\"Average confidence for incorrect predictions: {incorrect_confidences.mean():.4f}\")\n",
        "    print(f\"Min/Max confidence for incorrect: {incorrect_confidences.min():.4f}/{incorrect_confidences.max():.4f}\")\n",
        "\n",
        "# Analyze by sequence length\n",
        "print(f\"\\nüìè Performance by Sequence Length:\")\n",
        "length_bins = pd.qcut(results_df['sequence_length'], q=3, labels=['Short', 'Medium', 'Long'])\n",
        "length_analysis = results_df.groupby(length_bins).agg({\n",
        "    'correct': ['count', 'sum', 'mean'],\n",
        "    'confidence': 'mean',\n",
        "    'sequence_length': ['min', 'max']\n",
        "}).round(4)\n",
        "\n",
        "for length_cat in length_analysis.index:\n",
        "    count = length_analysis.loc[length_cat, ('correct', 'count')]\n",
        "    correct = length_analysis.loc[length_cat, ('correct', 'sum')]\n",
        "    acc = length_analysis.loc[length_cat, ('correct', 'mean')]\n",
        "    avg_conf = length_analysis.loc[length_cat, ('confidence', 'mean')]\n",
        "    min_len = length_analysis.loc[length_cat, ('sequence_length', 'min')]\n",
        "    max_len = length_analysis.loc[length_cat, ('sequence_length', 'max')]\n",
        "    print(f\"  {length_cat:6s} ({min_len:3.0f}-{max_len:3.0f} aa): {correct:2.0f}/{count:2.0f} = {acc:.3f} accuracy, {avg_conf:.3f} avg confidence\")\n",
        "\n",
        "# Family-level analysis\n",
        "family_performance = results_df.groupby('true_family').agg({\n",
        "    'correct': ['count', 'sum', 'mean'],\n",
        "    'confidence': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "families_with_multiple = family_performance[family_performance[('correct', 'count')] > 1]\n",
        "if len(families_with_multiple) > 0:\n",
        "    print(f\"\\nüß¨ Families with Multiple Test Sequences:\")\n",
        "    for family in families_with_multiple.index:\n",
        "        count = family_performance.loc[family, ('correct', 'count')]\n",
        "        correct = family_performance.loc[family, ('correct', 'sum')]\n",
        "        acc = family_performance.loc[family, ('correct', 'mean')]\n",
        "        avg_conf = family_performance.loc[family, ('confidence', 'mean')]\n",
        "        print(f\"  {family}: {correct:.0f}/{count:.0f} = {acc:.3f} accuracy, {avg_conf:.3f} avg confidence\")"
      ],
      "metadata": {
        "id": "BoGTzduwXGPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comprehensive visualizations for all test sequences\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# 1. Overall accuracy pie chart\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "correct_count = results_df['correct'].sum()\n",
        "incorrect_count = len(results_df) - correct_count\n",
        "ax1.pie([correct_count, incorrect_count],\n",
        "        labels=[f'Correct ({correct_count})', f'Incorrect ({incorrect_count})'],\n",
        "        colors=['lightgreen', 'lightcoral'],\n",
        "        autopct='%1.1f%%')\n",
        "ax1.set_title(f'Overall Accuracy\\n{len(results_df)} Test Sequences')\n",
        "\n",
        "# 2. Confidence distribution\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "ax2.hist([correct_confidences, incorrect_confidences],\n",
        "         bins=20, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "ax2.set_title('Confidence Distribution')\n",
        "ax2.set_xlabel('Confidence Score')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Sequence length vs confidence\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "scatter_colors = ['green' if correct else 'red' for correct in results_df['correct']]\n",
        "ax3.scatter(results_df['sequence_length'], results_df['confidence'],\n",
        "            c=scatter_colors, alpha=0.7)\n",
        "ax3.set_xlabel('Sequence Length (amino acids)')\n",
        "ax3.set_ylabel('Confidence')\n",
        "ax3.set_title('Sequence Length vs Confidence')\n",
        "\n",
        "# 4. Accuracy by sequence length bins\n",
        "ax4 = plt.subplot(3, 3, 4)\n",
        "length_bins = pd.qcut(results_df['sequence_length'], q=5, labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long'])\n",
        "length_accuracy = results_df.groupby(length_bins)['correct'].mean()\n",
        "ax4.bar(range(len(length_accuracy)), length_accuracy.values)\n",
        "ax4.set_xticks(range(len(length_accuracy)))\n",
        "ax4.set_xticklabels(length_accuracy.index, rotation=45)\n",
        "ax4.set_title('Accuracy by Sequence Length')\n",
        "ax4.set_ylabel('Accuracy')\n",
        "ax4.set_ylim(0, 1)\n",
        "\n",
        "# 5. Confidence vs accuracy scatter\n",
        "ax5 = plt.subplot(3, 3, 5)\n",
        "confidence_bins = pd.cut(results_df['confidence'], bins=10)\n",
        "conf_accuracy = results_df.groupby(confidence_bins)['correct'].mean()\n",
        "conf_centers = [interval.mid for interval in conf_accuracy.index]\n",
        "ax5.plot(conf_centers, conf_accuracy.values, 'bo-')\n",
        "ax5.set_xlabel('Confidence Score')\n",
        "ax5.set_ylabel('Accuracy')\n",
        "ax5.set_title('Calibration: Confidence vs Accuracy')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Top families in test set\n",
        "ax6 = plt.subplot(3, 3, 6)\n",
        "top_families = results_df['true_family'].value_counts().head(10)\n",
        "if len(top_families) > 0:\n",
        "    ax6.barh(range(len(top_families)), top_families.values)\n",
        "    ax6.set_yticks(range(len(top_families)))\n",
        "    ax6.set_yticklabels([f\"{fam[:15]}...\" if len(fam) > 15 else fam for fam in top_families.index])\n",
        "    ax6.set_title('Most Frequent Families in Test Set')\n",
        "    ax6.set_xlabel('Count')\n",
        "\n",
        "# 7. Confidence histogram by correctness\n",
        "ax7 = plt.subplot(3, 3, 7)\n",
        "ax7.hist(results_df['confidence'], bins=30, alpha=0.7, color='blue', label='All Predictions')\n",
        "if len(correct_confidences) > 0:\n",
        "    ax7.axvline(correct_confidences.mean(), color='green', linestyle='--', label=f'Correct Mean: {correct_confidences.mean():.3f}')\n",
        "if len(incorrect_confidences) > 0:\n",
        "    ax7.axvline(incorrect_confidences.mean(), color='red', linestyle='--', label=f'Incorrect Mean: {incorrect_confidences.mean():.3f}')\n",
        "ax7.set_title('Confidence Distribution with Means')\n",
        "ax7.set_xlabel('Confidence')\n",
        "ax7.set_ylabel('Count')\n",
        "ax7.legend()\n",
        "\n",
        "# 8. Sequence length distribution\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "ax8.hist(results_df['sequence_length'], bins=20, alpha=0.7, color='purple')\n",
        "ax8.axvline(results_df['sequence_length'].mean(), color='red', linestyle='--',\n",
        "            label=f'Mean: {results_df[\"sequence_length\"].mean():.0f}')\n",
        "ax8.set_title('Test Set Sequence Length Distribution')\n",
        "ax8.set_xlabel('Sequence Length (amino acids)')\n",
        "ax8.set_ylabel('Count')\n",
        "ax8.legend()\n",
        "\n",
        "# 9. Cumulative accuracy by confidence threshold\n",
        "ax9 = plt.subplot(3, 3, 9)\n",
        "confidence_thresholds = np.arange(0.1, 1.0, 0.05)\n",
        "cumulative_accuracy = []\n",
        "cumulative_count = []\n",
        "\n",
        "for threshold in confidence_thresholds:\n",
        "    above_threshold = results_df[results_df['confidence'] >= threshold]\n",
        "    if len(above_threshold) > 0:\n",
        "        cumulative_accuracy.append(above_threshold['correct'].mean())\n",
        "        cumulative_count.append(len(above_threshold))\n",
        "    else:\n",
        "        cumulative_accuracy.append(0)\n",
        "        cumulative_count.append(0)\n",
        "\n",
        "ax9_twin = ax9.twinx()\n",
        "line1 = ax9.plot(confidence_thresholds, cumulative_accuracy, 'g-o', label='Accuracy')\n",
        "line2 = ax9_twin.plot(confidence_thresholds, cumulative_count, 'b-s', alpha=0.7, label='Count')\n",
        "\n",
        "ax9.set_xlabel('Confidence Threshold')\n",
        "ax9.set_ylabel('Accuracy', color='g')\n",
        "ax9_twin.set_ylabel('Number of Predictions', color='b')\n",
        "ax9.set_title('Accuracy vs Count by Confidence Threshold')\n",
        "ax9.grid(True, alpha=0.3)\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax9.get_legend_handles_labels()\n",
        "lines2, labels2 = ax9_twin.get_legend_handles_labels()\n",
        "ax9.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nüèÜ COMPREHENSIVE EVALUATION SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Test Sequences: {len(results_df)}\")\n",
        "print(f\"‚úÖ Overall Accuracy: {accuracy:.4f}\")\n",
        "print(f\"‚úÖ Correct Predictions: {correct_count}\")\n",
        "print(f\"‚úÖ Average Confidence: {results_df['confidence'].mean():.4f}\")\n",
        "print(f\"‚úÖ High Confidence Predictions (‚â•0.8): {high_confidence} ({high_confidence/len(results_df)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Sequence Length Range: {results_df['sequence_length'].min()}-{results_df['sequence_length'].max()} amino acids\")"
      ],
      "metadata": {
        "id": "XTomoEPkYDu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed top-k analysis using direct model inference\n",
        "def get_top_k_predictions_direct(sequence, k=5):\n",
        "    \"\"\"Get top-k predictions using direct model inference\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        sequence,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities[0], k)\n",
        "\n",
        "    top_k_families = []\n",
        "    for prob, idx in zip(top_k_probs, top_k_indices):\n",
        "        family = label_encoder.classes_[idx.item()]\n",
        "        top_k_families.append((family, prob.item()))\n",
        "\n",
        "    return top_k_families\n",
        "\n",
        "# CONFIGURABLE: Set how many sequences to analyze for top-k (can be expensive)\n",
        "TOP_K_SAMPLE_SIZE = min(20, len(test_sequences_final))  # üéØ CHANGE THIS NUMBER\n",
        "\n",
        "print(f\"üîç Analyzing Top-K Accuracy on {TOP_K_SAMPLE_SIZE} sequences...\")\n",
        "\n",
        "# Calculate top-k accuracy for k=1,3,5,10\n",
        "k_values = [1, 3, 5, 10]\n",
        "top_k_accuracies = {}\n",
        "\n",
        "for k in k_values:\n",
        "    correct_in_top_k = 0\n",
        "\n",
        "    print(f\"Calculating Top-{k} accuracy...\", end='')\n",
        "    for i in range(TOP_K_SAMPLE_SIZE):\n",
        "        sequence = test_sequences_final[i]\n",
        "        true_family = true_families[i]\n",
        "\n",
        "        top_k_preds = get_top_k_predictions_direct(sequence, k)\n",
        "        top_k_families = [family for family, score in top_k_preds]\n",
        "\n",
        "        if true_family in top_k_families:\n",
        "            correct_in_top_k += 1\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f\".\", end='')\n",
        "\n",
        "    top_k_accuracies[k] = correct_in_top_k / TOP_K_SAMPLE_SIZE\n",
        "    print(f\" Done!\")\n",
        "\n",
        "print(f\"\\nüìä Top-K Accuracy Results ({TOP_K_SAMPLE_SIZE} sequences):\")\n",
        "print(\"-\" * 40)\n",
        "for k, acc in top_k_accuracies.items():\n",
        "    print(f\"  Top-{k:2d} Accuracy: {acc:.4f} ({acc*100:.1f}%)\")\n",
        "\n",
        "# Show detailed top-5 predictions for first few sequences\n",
        "num_detailed_examples = min(3, len(test_sequences_final))\n",
        "print(f\"\\nüî¨ Detailed Top-5 Predictions for First {num_detailed_examples} Sequences:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for seq_idx in range(num_detailed_examples):\n",
        "    print(f\"\\nSequence {seq_idx + 1}:\")\n",
        "    print(f\"True family: {true_families[seq_idx]}\")\n",
        "    print(f\"Length: {len(test_sequences_final[seq_idx])} amino acids\")\n",
        "    print(f\"Sequence: {test_sequences_final[seq_idx][:60]}...\")\n",
        "\n",
        "    top_5 = get_top_k_predictions_direct(test_sequences_final[seq_idx], 5)\n",
        "    print(f\"Top-5 Predictions:\")\n",
        "    for i, (family, confidence) in enumerate(top_5, 1):\n",
        "        marker = \"‚úÖ\" if family == true_families[seq_idx] else \"  \"\n",
        "        print(f\"  {marker} {i}. {family} (confidence: {confidence:.4f})\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Top-k accuracy visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, [top_k_accuracies[k] for k in k_values], 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('K (Top-K)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'Top-K Accuracy Analysis\\n({TOP_K_SAMPLE_SIZE} test sequences)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on points\n",
        "for k in k_values:\n",
        "    plt.annotate(f'{top_k_accuracies[k]:.3f}',\n",
        "                (k, top_k_accuracies[k]),\n",
        "                textcoords=\"offset points\",\n",
        "                xytext=(0,10),\n",
        "                ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nkKkZYlDYEMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Within Family Generalization Evals\n"
      ],
      "metadata": {
        "id": "V4LwcaN1pbuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üéØ Creating Within-Family Generalization Test Set\")\n",
        "print(\"=\"*60)\n",
        "print(\"Testing model's ability to generalize to unseen sequences from KNOWN families\")\n",
        "\n",
        "# Strategy: Get sequences from the top 1,000 families that were NOT used in training\n",
        "training_families = set(original_top_1000_families)\n",
        "print(f\"üìä Target families: {len(training_families)} (same families model was trained on)\")\n",
        "\n",
        "# Get all sequences from these families\n",
        "within_family_candidates = all_data_for_search[\n",
        "    all_data_for_search['family_accession'].isin(training_families)\n",
        "]\n",
        "print(f\"Total sequences from target families: {len(within_family_candidates):,}\")\n",
        "\n",
        "# Exclude sequences that were used in training\n",
        "within_family_test_candidates = within_family_candidates[\n",
        "    ~within_family_candidates['sequence'].isin(original_training_sequences)\n",
        "]\n",
        "\n",
        "print(f\"üìä Within-Family Test Filtering:\")\n",
        "print(f\"  Sequences from target families: {len(within_family_candidates):,}\")\n",
        "print(f\"  Training sequences to exclude: {len(original_training_sequences):,}\")\n",
        "print(f\"  Clean within-family candidates: {len(within_family_test_candidates):,}\")\n",
        "\n",
        "if len(within_family_test_candidates) == 0:\n",
        "    print(\"‚ùå No within-family test candidates available!\")\n",
        "else:\n",
        "    # Create balanced test set - sample from each family\n",
        "    target_test_size = 100  # üéØ CONFIGURABLE: Change this number\n",
        "\n",
        "    # Strategy: Sample evenly from families that have enough sequences\n",
        "    family_test_counts = within_family_test_candidates['family_accession'].value_counts()\n",
        "    families_with_data = family_test_counts[family_test_counts > 0]\n",
        "\n",
        "    print(f\"\\nüéØ Test Set Creation Strategy:\")\n",
        "    print(f\"  Target test size: {target_test_size}\")\n",
        "    print(f\"  Families with available test data: {len(families_with_data)}\")\n",
        "    print(f\"  Average available per family: {families_with_data.mean():.1f}\")\n",
        "\n",
        "    # Sample strategy: aim for roughly equal representation\n",
        "    sequences_per_family = max(1, target_test_size // len(families_with_data))\n",
        "    remaining_slots = target_test_size % len(families_with_data)\n",
        "\n",
        "    print(f\"  Base sequences per family: {sequences_per_family}\")\n",
        "    print(f\"  Extra sequences for top families: {remaining_slots}\")\n",
        "\n",
        "    within_family_test_list = []\n",
        "    families_sampled = 0\n",
        "\n",
        "    # Sort families by available test sequences (most first)\n",
        "    sorted_families = families_with_data.sort_values(ascending=False)\n",
        "\n",
        "    for family in sorted_families.index:\n",
        "        if len(within_family_test_list) >= target_test_size:\n",
        "            break\n",
        "\n",
        "        family_data = within_family_test_candidates[\n",
        "            within_family_test_candidates['family_accession'] == family\n",
        "        ]\n",
        "\n",
        "        # Sample size: base + extra for top families\n",
        "        sample_size = sequences_per_family\n",
        "        if families_sampled < remaining_slots:\n",
        "            sample_size += 1\n",
        "\n",
        "        # Don't exceed available sequences\n",
        "        sample_size = min(sample_size, len(family_data))\n",
        "\n",
        "        if sample_size > 0:\n",
        "            sampled = family_data.sample(n=sample_size, random_state=42)\n",
        "            for _, row in sampled.iterrows():\n",
        "                within_family_test_list.append(row)\n",
        "            families_sampled += 1\n",
        "\n",
        "    # Create test DataFrame\n",
        "    within_family_test_df = pd.DataFrame(within_family_test_list)\n",
        "\n",
        "    # Limit to target size if we went over\n",
        "    if len(within_family_test_df) > target_test_size:\n",
        "        within_family_test_df = within_family_test_df.sample(\n",
        "            n=target_test_size, random_state=42\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n‚úÖ Within-Family Test Set Created:\")\n",
        "    print(f\"  Total test sequences: {len(within_family_test_df)}\")\n",
        "    print(f\"  Unique families: {within_family_test_df['family_accession'].nunique()}\")\n",
        "    print(f\"  Families represented: {within_family_test_df['family_accession'].nunique()}/{len(training_families)}\")\n",
        "\n",
        "    # Analyze test set composition\n",
        "    family_distribution = within_family_test_df['family_accession'].value_counts()\n",
        "    print(f\"  Sequences per family range: {family_distribution.min()}-{family_distribution.max()}\")\n",
        "    print(f\"  Average sequences per family: {family_distribution.mean():.1f}\")\n",
        "\n",
        "    # Verify NO overlap with training\n",
        "    overlap_check = within_family_test_df['sequence'].isin(original_training_sequences).sum()\n",
        "    print(f\"üîç CRITICAL VERIFICATION: {overlap_check} sequences overlap with training (MUST be 0)\")\n",
        "\n",
        "    if overlap_check == 0:\n",
        "        print(\"‚úÖ SUCCESS: Within-family test set with no training overlap!\")\n",
        "    else:\n",
        "        print(\"‚ùå ERROR: Test set contains training sequences!\")\n",
        "\n",
        "    # Prepare test data\n",
        "    within_family_valid_test_data = [\n",
        "        (row['sequence'], row['family_accession'])\n",
        "        for _, row in within_family_test_df.iterrows()\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nüìà Expected Performance:\")\n",
        "    print(\"  This test evaluates WITHIN-FAMILY generalization\")\n",
        "    print(\"  Model should perform MUCH better on these sequences\")\n",
        "    print(\"  Tests if model learned family patterns vs. memorized sequences\")\n",
        "\n",
        "    # Save the within-family test set\n",
        "    WITHIN_FAMILY_TEST_FILE = 'within_family_test_data.pkl'\n",
        "    within_family_cache = {\n",
        "        'test_df': within_family_test_df,\n",
        "        'valid_test_data': within_family_valid_test_data,\n",
        "        'family_distribution': family_distribution\n",
        "    }\n",
        "\n",
        "    with open(WITHIN_FAMILY_TEST_FILE, 'wb') as f:\n",
        "        pickle.dump(within_family_cache, f)\n",
        "\n",
        "    print(f\"üíæ Within-family test set saved to: {WITHIN_FAMILY_TEST_FILE}\")"
      ],
      "metadata": {
        "id": "fw5sIJPJYJa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "within_family_test_df.to_csv(\"within_family_test_set.csv\", index=False)"
      ],
      "metadata": {
        "id": "L8ambi7gjsZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the within-family test data\n",
        "test_sequences_within_family = [item[0] for item in within_family_valid_test_data]\n",
        "true_families_within_family = [item[1] for item in within_family_valid_test_data]\n",
        "\n",
        "print(f\"üî¨ Running Within-Family Generalization Test\")\n",
        "print(f\"Testing on {len(test_sequences_within_family)} sequences from KNOWN families\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "predictions_within_family = []\n",
        "prediction_confidences_within_family = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, sequence in enumerate(test_sequences_within_family):\n",
        "        print(f\"Predicting sequence {i+1}/{len(test_sequences_within_family)}...\", end='\\r')\n",
        "\n",
        "        # Tokenize the sequence\n",
        "        inputs = tokenizer(\n",
        "            sequence,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Get probabilities and prediction\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_label_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "        confidence = probabilities[0, predicted_label_idx].item()\n",
        "\n",
        "        # Convert to family name\n",
        "        predicted_family = label_encoder.classes_[predicted_label_idx]\n",
        "\n",
        "        predictions_within_family.append(predicted_family)\n",
        "        prediction_confidences_within_family.append(confidence)\n",
        "\n",
        "print(f\"\\n‚úÖ Within-family predictions complete!\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_within_family_df = pd.DataFrame({\n",
        "    'sequence': test_sequences_within_family,\n",
        "    'true_family': true_families_within_family,\n",
        "    'predicted_family': predictions_within_family,\n",
        "    'confidence': prediction_confidences_within_family,\n",
        "    'sequence_length': [len(seq) for seq in test_sequences_within_family]\n",
        "})\n",
        "\n",
        "# Add correctness column\n",
        "results_within_family_df['correct'] = (\n",
        "    results_within_family_df['true_family'] == results_within_family_df['predicted_family']\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Within-Family Generalization Results:\")\n",
        "print(f\"Total predictions: {len(results_within_family_df)}\")\n",
        "print(f\"Correct predictions: {results_within_family_df['correct'].sum()}\")\n",
        "print(f\"Accuracy: {results_within_family_df['correct'].mean():.4f}\")\n",
        "print(f\"Average confidence: {results_within_family_df['confidence'].mean():.4f}\")\n",
        "\n",
        "# Compare with previous results (if available)\n",
        "if 'results_df' in globals():\n",
        "    previous_accuracy = results_df['correct'].mean()\n",
        "    improvement = results_within_family_df['correct'].mean() - previous_accuracy\n",
        "    print(f\"\\nüìà Performance Comparison:\")\n",
        "    print(f\"Previous test (unknown families): {previous_accuracy:.4f}\")\n",
        "    print(f\"Within-family test (known families): {results_within_family_df['correct'].mean():.4f}\")\n",
        "    print(f\"Improvement: {improvement:+.4f} ({improvement*100:+.1f} percentage points)\")"
      ],
      "metadata": {
        "id": "YVk3iqXXhzXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed analysis of within-family performance\n",
        "print(f\"üéØ DETAILED WITHIN-FAMILY GENERALIZATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "within_family_accuracy = results_within_family_df['correct'].mean()\n",
        "within_family_confidence = results_within_family_df['confidence'].mean()\n",
        "\n",
        "print(f\"Overall Within-Family Accuracy: {within_family_accuracy:.4f} ({within_family_accuracy*100:.1f}%)\")\n",
        "print(f\"Average Confidence: {within_family_confidence:.4f}\")\n",
        "\n",
        "# Analyze performance by family\n",
        "family_performance = results_within_family_df.groupby('true_family').agg({\n",
        "    'correct': ['count', 'sum', 'mean'],\n",
        "    'confidence': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "print(f\"\\nüìä Performance by Family:\")\n",
        "print(f\"Families tested: {len(family_performance)}\")\n",
        "\n",
        "# Show families with perfect accuracy\n",
        "perfect_families = family_performance[family_performance[('correct', 'mean')] == 1.0]\n",
        "if len(perfect_families) > 0:\n",
        "    print(f\"\\nüéØ Families with Perfect Accuracy ({len(perfect_families)}):\")\n",
        "    for family in perfect_families.index[:10]:  # Show first 10\n",
        "        count = family_performance.loc[family, ('correct', 'count')]\n",
        "        conf = family_performance.loc[family, ('confidence', 'mean')]\n",
        "        print(f\"  {family}: {count:.0f}/{count:.0f} (confidence: {conf:.3f})\")\n",
        "    if len(perfect_families) > 10:\n",
        "        print(f\"  ... and {len(perfect_families) - 10} more\")\n",
        "\n",
        "# Show families with poor performance\n",
        "poor_families = family_performance[family_performance[('correct', 'mean')] < 0.5]\n",
        "if len(poor_families) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è Families with <50% Accuracy ({len(poor_families)}):\")\n",
        "    for family in poor_families.index[:10]:\n",
        "        count = family_performance.loc[family, ('correct', 'count')]\n",
        "        correct = family_performance.loc[family, ('correct', 'sum')]\n",
        "        acc = family_performance.loc[family, ('correct', 'mean')]\n",
        "        conf = family_performance.loc[family, ('confidence', 'mean')]\n",
        "        print(f\"  {family}: {correct:.0f}/{count:.0f} = {acc:.3f} (confidence: {conf:.3f})\")\n",
        "\n",
        "# Confidence analysis\n",
        "correct_confidences_wf = results_within_family_df[results_within_family_df['correct']]['confidence']\n",
        "incorrect_confidences_wf = results_within_family_df[~results_within_family_df['correct']]['confidence']\n",
        "\n",
        "print(f\"\\nüìä Confidence Analysis:\")\n",
        "if len(correct_confidences_wf) > 0:\n",
        "    print(f\"Correct predictions confidence: {correct_confidences_wf.mean():.4f} ¬± {correct_confidences_wf.std():.4f}\")\n",
        "if len(incorrect_confidences_wf) > 0:\n",
        "    print(f\"Incorrect predictions confidence: {incorrect_confidences_wf.mean():.4f} ¬± {incorrect_confidences_wf.std():.4f}\")\n",
        "\n",
        "# High confidence analysis\n",
        "high_conf_threshold = 0.9\n",
        "high_conf_predictions = results_within_family_df[results_within_family_df['confidence'] >= high_conf_threshold]\n",
        "if len(high_conf_predictions) > 0:\n",
        "    high_conf_accuracy = high_conf_predictions['correct'].mean()\n",
        "    print(f\"\\nHigh confidence (‚â•{high_conf_threshold}) predictions:\")\n",
        "    print(f\"  Count: {len(high_conf_predictions)} ({len(high_conf_predictions)/len(results_within_family_df)*100:.1f}%)\")\n",
        "    print(f\"  Accuracy: {high_conf_accuracy:.4f}\")\n",
        "\n",
        "# Examples of predictions\n",
        "print(f\"\\nüîç Example Within-Family Predictions:\")\n",
        "print(\"-\" * 80)\n",
        "for i in range(min(5, len(results_within_family_df))):\n",
        "    row = results_within_family_df.iloc[i]\n",
        "    status = \"‚úÖ CORRECT\" if row['correct'] else \"‚ùå INCORRECT\"\n",
        "    print(f\"Example {i+1}: {status}\")\n",
        "    print(f\"  Family: {row['true_family']}\")\n",
        "    print(f\"  Predicted: {row['predicted_family']}\")\n",
        "    print(f\"  Confidence: {row['confidence']:.4f}\")\n",
        "    print(f\"  Sequence ({row['sequence_length']} aa): {row['sequence'][:60]}...\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "-eg_sWzpiBZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparative visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Accuracy comparison\n",
        "if 'results_df' in globals():\n",
        "    accuracies = [results_df['correct'].mean(), results_within_family_df['correct'].mean()]\n",
        "    labels = ['Unknown Families\\n(Out-of-domain)', 'Known Families\\n(Within-domain)']\n",
        "    colors = ['lightcoral', 'lightgreen']\n",
        "else:\n",
        "    accuracies = [results_within_family_df['correct'].mean()]\n",
        "    labels = ['Known Families\\n(Within-domain)']\n",
        "    colors = ['lightgreen']\n",
        "\n",
        "axes[0, 0].bar(labels, accuracies, color=colors)\n",
        "axes[0, 0].set_title('Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    axes[0, 0].text(i, acc + 0.02, f'{acc:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# 2. Confidence comparison\n",
        "if 'results_df' in globals():\n",
        "    confidences = [results_df['confidence'].mean(), results_within_family_df['confidence'].mean()]\n",
        "    axes[0, 1].bar(labels, confidences, color=colors)\n",
        "else:\n",
        "    confidences = [results_within_family_df['confidence'].mean()]\n",
        "    axes[0, 1].bar(labels, confidences, color=colors)\n",
        "\n",
        "axes[0, 1].set_title('Average Confidence Comparison')\n",
        "axes[0, 1].set_ylabel('Confidence')\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "for i, conf in enumerate(confidences):\n",
        "    axes[0, 1].text(i, conf + 0.02, f'{conf:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# 3. Within-family confidence distribution\n",
        "axes[0, 2].hist([correct_confidences_wf, incorrect_confidences_wf],\n",
        "                bins=20, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "axes[0, 2].set_title('Within-Family Confidence Distribution')\n",
        "axes[0, 2].set_xlabel('Confidence')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# 4. Family-level performance distribution\n",
        "family_accuracies = family_performance[('correct', 'mean')].values\n",
        "axes[1, 0].hist(family_accuracies, bins=20, alpha=0.7, color='blue')\n",
        "axes[1, 0].set_title('Distribution of Family-Level Accuracies')\n",
        "axes[1, 0].set_xlabel('Accuracy per Family')\n",
        "axes[1, 0].set_ylabel('Number of Families')\n",
        "axes[1, 0].axvline(family_accuracies.mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {family_accuracies.mean():.3f}')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. Sequence length vs accuracy (within-family)\n",
        "length_bins = pd.qcut(results_within_family_df['sequence_length'], q=5, labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long'])\n",
        "length_accuracy_wf = results_within_family_df.groupby(length_bins)['correct'].mean()\n",
        "axes[1, 1].bar(range(len(length_accuracy_wf)), length_accuracy_wf.values, color='green', alpha=0.7)\n",
        "axes[1, 1].set_xticks(range(len(length_accuracy_wf)))\n",
        "axes[1, 1].set_xticklabels(length_accuracy_wf.index, rotation=45)\n",
        "axes[1, 1].set_title('Within-Family Accuracy by Sequence Length')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "\n",
        "# 6. Top/bottom performing families\n",
        "top_families = family_performance.nlargest(10, ('correct', 'mean'))\n",
        "bottom_families = family_performance.nsmallest(10, ('correct', 'mean'))\n",
        "\n",
        "y_pos_top = range(len(top_families))\n",
        "axes[1, 2].barh(y_pos_top, top_families[('correct', 'mean')], color='green', alpha=0.7)\n",
        "axes[1, 2].set_yticks(y_pos_top)\n",
        "axes[1, 2].set_yticklabels([f\"{fam[:20]}...\" if len(fam) > 20 else fam for fam in top_families.index])\n",
        "axes[1, 2].set_title('Top 10 Performing Families')\n",
        "axes[1, 2].set_xlabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nüèÜ WITHIN-FAMILY GENERALIZATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Test Type: Within-Family Generalization\")\n",
        "print(f\"‚úÖ Test Size: {len(results_within_family_df)} sequences\")\n",
        "print(f\"‚úÖ Families Tested: {results_within_family_df['true_family'].nunique()}\")\n",
        "print(f\"‚úÖ Overall Accuracy: {within_family_accuracy:.4f} ({within_family_accuracy*100:.1f}%)\")\n",
        "print(f\"‚úÖ Average Confidence: {within_family_confidence:.4f}\")\n",
        "print(f\"‚úÖ Perfect Families: {len(perfect_families)} ({len(perfect_families)/len(family_performance)*100:.1f}%)\")\n",
        "\n",
        "if 'results_df' in globals():\n",
        "    improvement = within_family_accuracy - results_df['correct'].mean()\n",
        "    print(f\"‚úÖ Improvement over unknown families: {improvement:+.4f} ({improvement*100:+.1f}%)\")\n",
        "\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ Model shows strong within-family generalization\")\n",
        "print(\"‚Ä¢ Can learn family patterns beyond memorizing specific sequences\")\n",
        "print(\"‚Ä¢ Performance varies significantly across families\")\n",
        "print(\"‚Ä¢ High confidence predictions are highly reliable\")\n",
        "\n",
        "# Save within-family results\n",
        "results_within_family_df.to_csv('within_family_evaluation_results.csv', index=False)\n",
        "print(f\"\\nüíæ Within-family results saved to: within_family_evaluation_results.csv\")"
      ],
      "metadata": {
        "id": "g5qKd6IoiGmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "docEoD0hrRu9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}