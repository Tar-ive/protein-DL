{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPijVgR+xAl/1HfdQNqT303",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/protein-DL/blob/main/amino_acid_hack_nation_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVFeyv7HD1WQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"googleai/pfam-seed-random-split\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "TAdEf-vgEPeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "rcEgOdylEcg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check what's actually in the dataset directory\n",
        "print(\"Contents of dataset directory:\")\n",
        "for item in os.listdir(path):\n",
        "    print(f\"  {item}\")\n",
        "    if os.path.isdir(os.path.join(path, item)):\n",
        "        print(f\"    Contents of {item}:\")\n",
        "        for subitem in os.listdir(os.path.join(path, item)):\n",
        "            print(f\"      {subitem}\")"
      ],
      "metadata": {
        "id": "uxg-H_S3EZUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_path = os.path.join(path, 'random_split', 'random_split')\n"
      ],
      "metadata": {
        "id": "dVdxYDulOffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_sharded_files(subdir_name, base_path):\n",
        "    \"\"\"Read all sharded data files from a subdirectory and concatenate them\"\"\"\n",
        "    dir_path = os.path.join(base_path, subdir_name)\n",
        "    data_frames = []\n",
        "\n",
        "    # Get all files and sort them to maintain order\n",
        "    files = sorted([f for f in os.listdir(dir_path) if f.startswith('data-')])\n",
        "\n",
        "    for file in files:\n",
        "        file_path = os.path.join(dir_path, file)\n",
        "        try:\n",
        "            # Try reading as parquet first (most likely format)\n",
        "            df = pd.read_parquet(file_path)\n",
        "            data_frames.append(df)\n",
        "        except:\n",
        "            try:\n",
        "                # If parquet fails, try as CSV\n",
        "                df = pd.read_csv(file_path)\n",
        "                data_frames.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read {file}: {e}\")\n",
        "\n",
        "    if data_frames:\n",
        "        return pd.concat(data_frames, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"No readable files found in {dir_path}\")\n",
        "        return None\n",
        "\n",
        "# Use the new function to load your data\n",
        "train = read_data_from_sharded_files('train', inner_path)\n",
        "dev = read_data_from_sharded_files('dev', inner_path)\n",
        "test = read_data_from_sharded_files('test', inner_path)\n",
        "\n",
        "print(f\"Train shape: {train.shape if train is not None else 'Failed to load'}\")\n",
        "print(f\"Dev shape: {dev.shape if dev is not None else 'Failed to load'}\")\n",
        "print(f\"Test shape: {test.shape if test is not None else 'Failed to load'}\")"
      ],
      "metadata": {
        "id": "zIvn7BR1Ed4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()\n"
      ],
      "metadata": {
        "id": "XPTA_z-CE6Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "1tczOIu1E-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev.shape"
      ],
      "metadata": {
        "id": "6i4w7XtwE-uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "CELUOkwXE_-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at families in the training data"
      ],
      "metadata": {
        "id": "YOeUCYsZFR-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partitions = {'train': train, 'dev': dev, 'test': test}\n"
      ],
      "metadata": {
        "id": "Iz8ugCYnF0e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_information(partitions):\n",
        "    columns = ['partition', 'nb_samples', 'nb_families', 'min_samples_per_fam', 'max_samples_per_fam', 'mean_samples_per_fam']\n",
        "    df_info = pd.DataFrame(columns=columns)\n",
        "    for name, df in partitions.items():\n",
        "        # Use pd.concat instead of df.append\n",
        "        df_info = pd.concat([df_info, pd.DataFrame([{\n",
        "            'partition': name,\n",
        "            'nb_samples': len(df),\n",
        "            'nb_families': df['family_accession'].unique().size,\n",
        "            'max_samples_per_fam': df.groupby('family_accession').size().max(),\n",
        "            'min_samples_per_fam': df.groupby('family_accession').size().min(),\n",
        "            'mean_samples_per_fam': df.groupby('family_accession').size().mean(),\n",
        "        }])], ignore_index=True)\n",
        "    return df_info\n",
        "\n",
        "get_information(partitions)"
      ],
      "metadata": {
        "id": "mS99cQbnFx0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_families = set(train['family_accession'].unique())\n",
        "dev_families = set(dev['family_accession'].unique())\n",
        "test_families = set(test['family_accession'].unique())\n",
        "print('Are the families of the dev set and the test set the same ?', dev_families == test_families)\n",
        "\n",
        "common_families = train_families & dev_families & test_families # Take the intersection with the '&' operator\n",
        "print('Number of common families in all sets : ', len(common_families))"
      ],
      "metadata": {
        "id": "Qo8jPPBaFAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excluding the families that are only in train but not in dev and test\n"
      ],
      "metadata": {
        "id": "UfQpvPOTFizj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train['family_accession'].isin(common_families)]\n",
        "partitions['train'] = train\n",
        "\n",
        "print('Updated info on the datasets')\n",
        "get_information(partitions)"
      ],
      "metadata": {
        "id": "AbJGnMn6FiJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (30, 10))\n",
        "plt.suptitle('Distribution of family sizes', fontsize=18, y=0.95)\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "for n, (name, df) in enumerate(partitions.items()):\n",
        "    # Create the subpot\n",
        "    ax = plt.subplot(1, 3, n + 1)\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(\"Family size\")\n",
        "    ax.set_ylabel(\"Number of families\")\n",
        "\n",
        "    # Plot data\n",
        "    df.groupby('family_id').size().hist(bins=100, ax=ax, color=colors[n])\n"
      ],
      "metadata": {
        "id": "4cKgiXA9F9zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Environemnt Setup"
      ],
      "metadata": {
        "id": "1iHvDxb5Ko03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets evaluate scikit-learn\n"
      ],
      "metadata": {
        "id": "KT92TweGKsnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zJqHilZpKwem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "NzDzvYAsLI45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You already have this data loaded, so let's just verify it\n",
        "print(\"Data shapes:\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Dev: {dev.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "\n",
        "# Check the column names\n",
        "print(f\"\\nTrain columns: {train.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "8VeAFe4yLKos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all datasets for sampling strategy\n",
        "all_data = pd.concat([train, dev, test], ignore_index=True)\n",
        "print(f\"Total dataset size: {all_data.shape}\")\n",
        "\n",
        "# Explore the family distribution\n",
        "family_counts = all_data['family_accession'].value_counts()\n",
        "print(f\"Number of unique families: {len(family_counts)}\")\n",
        "print(f\"Most common families:\")\n",
        "print(family_counts.head(10))"
      ],
      "metadata": {
        "id": "UJC8gZYiLNUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed things up, I selected a smaller, representative sample. A good starting point was to take the top 1,000 most frequent families and then take up to 100 examples from each of those families. This gives me a balanced and manageable dataset of around 100,000 sequences"
      ],
      "metadata": {
        "id": "0Pkoobu2LRT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 1000 most frequent families\n",
        "top_1000_families = family_counts.head(1000).index.tolist()\n",
        "print(f\"Selected top {len(top_1000_families)} families\")\n",
        "\n",
        "# Filter data to only include these families\n",
        "filtered_data = all_data[all_data['family_accession'].isin(top_1000_families)]\n",
        "print(f\"Filtered dataset size: {filtered_data.shape}\")"
      ],
      "metadata": {
        "id": "X8NGva2hLfGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample up to 100 sequences per family for balanced training\n",
        "sampled_data = []\n",
        "\n",
        "for family in top_1000_families:\n",
        "    family_data = filtered_data[filtered_data['family_accession'] == family]\n",
        "    # Sample up to 100, or all if less than 100\n",
        "    sample_size = min(100, len(family_data))\n",
        "    sampled_family = family_data.sample(n=sample_size, random_state=42)\n",
        "    sampled_data.append(sampled_family)\n",
        "\n",
        "# Combine all sampled data\n",
        "balanced_dataset = pd.concat(sampled_data, ignore_index=True)\n",
        "print(f\"Balanced dataset size: {balanced_dataset.shape}\")\n",
        "print(f\"Average samples per family: {len(balanced_dataset) / len(top_1000_families):.1f}\")"
      ],
      "metadata": {
        "id": "XpFj8xZlLkFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sequences and labels\n",
        "sequences = balanced_dataset['sequence'].tolist()\n",
        "family_labels = balanced_dataset['family_accession'].tolist()\n",
        "\n",
        "print(f\"Number of sequences: {len(sequences)}\")\n",
        "print(f\"Number of labels: {len(family_labels)}\")\n",
        "print(f\"Example sequence length: {len(sequences[0])}\")\n",
        "print(f\"Example sequence: {sequences[0][:50]}...\")"
      ],
      "metadata": {
        "id": "l6QJZo8qLxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert family accession strings to numbers\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(family_labels)\n",
        "\n",
        "print(f\"Label encoding complete!\")\n",
        "print(f\"Number of unique labels: {len(label_encoder.classes_)}\")\n",
        "print(f\"Example mappings:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {family_labels[i]} -> {encoded_labels[i]}\")"
      ],
      "metadata": {
        "id": "Huca6TfEL3rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ESM-2 tokenizer\n",
        "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_checkpoint}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
      ],
      "metadata": {
        "id": "F3asofyDL7Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all sequences (this might take a few minutes)\n",
        "print(\"Tokenizing sequences...\")\n",
        "tokenized_sequences = tokenizer(\n",
        "    sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,  # Adjust if needed based on your sequence lengths\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"Input shape: {tokenized_sequences['input_ids'].shape}\")"
      ],
      "metadata": {
        "id": "gpcY7hVAMBvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets (80/20 split)\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    sequences,\n",
        "    encoded_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=encoded_labels  # Ensure balanced split across families\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_sequences)}\")\n",
        "print(f\"Test set size: {len(test_sequences)}\")"
      ],
      "metadata": {
        "id": "k46X5-MXMW5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the split data\n",
        "train_tokenized = tokenizer(\n",
        "    train_sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "test_tokenized = tokenizer(\n",
        "    test_sequences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "print(\"Split data tokenized!\")"
      ],
      "metadata": {
        "id": "ScmF8TDpMenV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_tokenized)\n",
        "test_dataset = Dataset.from_dict(test_tokenized)\n",
        "\n",
        "# Add labels\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels.tolist())\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels.tolist())\n",
        "\n",
        "print(\"Final datasets created!\")\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Test dataset: {test_dataset}\")\n",
        "print(f\"Number of labels: {len(label_encoder.classes_)}\")"
      ],
      "metadata": {
        "id": "yDSx_pQLMuk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ESM-2 model for sequence classification\n",
        "num_labels = 1000  # Your number of protein families\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "print(f\"Model loaded with {num_labels} output classes\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "b6UybGngMzc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face to enable automatic upload\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "esGBhs78NrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Create a descriptive model name\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "output_dir = f\"{model_name}-finetuned-pfam-1k\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,  # Adjust if you get memory errors\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,  # This will auto-upload to HF!\n",
        "    hub_model_id=f\"Tarive/{output_dir}\",  # Replace with your HF username\n",
        "    hub_strategy=\"every_save\",\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "print(f\"Training will save to: {output_dir}\")\n",
        "print(f\"Model will be uploaded to: Tarive/{output_dir}\")"
      ],
      "metadata": {
        "id": "J7i3bd_0Nu1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# Load accuracy metric\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined!\")"
      ],
      "metadata": {
        "id": "eNafEGo5N9My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer created! Ready to start training...\")"
      ],
      "metadata": {
        "id": "-R7GMqUHN_rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the big moment - start training!\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "print(\"This will take approximately 15-30 minutes on T4 GPU\")\n",
        "print(\"You'll see progress bars and accuracy metrics\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Training complete!\")"
      ],
      "metadata": {
        "id": "joo-8pfoOCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the label encoder to a file named 'label_encoder.pkl'\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"âœ… LabelEncoder saved to label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "riMVCyEQlCBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}